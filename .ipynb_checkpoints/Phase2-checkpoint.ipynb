{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Sunil Sabnis \n",
    "* Udai Khattar\n",
    "* Camden Wiseman\n",
    "* Chris Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several research questions. First, we are curious to know if Biden’s or Trump’s tweets are changing in some way as the election draws nearer. By “some way”, we mean if the length of the tweet, the number of favorites the tweet receives, the number of retweets the tweets have, the sentiment of each tweet, or any other data points about each tweet changes. Second, we want to use Twitter sentiment analysis to compare the two accounts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collected the data through the Twitter API, we search for a userID and the amount of tweets that we want, and the API gives us tweets back in the form of twitter objects. Not only do the tweets have the text, but they also have a load of other supplementary data. The twitter object is given back in JSON format. To create the dataset that we have, we chose a few of the data points that we wanted from each object and used lists to combine them into a dataframe. After doing this we had a dataframe generated that contained all of the data attributes that we wanted. The data points that we kept for our dataframe are: the id number, the full text, entities data, date and time created, favourites, retweets, language, geo location and who, if anyone, the tweet was in reply to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collection idea was inspired by a data analysis project by Eric Bruin posted on Kaggle that compared tweets between Donald Trump and Hilary Clinton in 2016 (Bruin). Similar to Bruin, we are interested in looking at “real social media data” and looking for relationships within the data. Bruin wrote a script in R to extract tweets from each account. We developed a similar approach to mine the tweets. First, we had to sign up for a Twitter Developer account in order to gain access to Twitter API keys. Once our applications were approved, we installed the Tweepy API. The Tweepy API allowed us to call the Twitter API functions within a Jupyter notebook using Python. From there, we developed our own script that was able to extract tweets from a given user. We collected 3083 tweets from Joe Biden and 1661 tweets from Donald Trump. We hope to collect more tweets from Trump’s twitter account. Unfortunately when mining Trump’s account, the API stalled once it reached ~1600 tweets. The script created an array of tweets. Then using the pandas dataframe, we wrote a CSV file that contained all the data. Each observation represents a tweet; during the collection phase we specifically filtered out retweets so that the dataset would only contain tweets generated by the user. Each tweet has an id, full text, entities (ex: hashtags), created at date, number of favorites (likes), number of retweets, language, geolocation (if available), and an in reply attribute (indicates if a tweet was in response to another account). The Twitter API allows for a maximum collection of 3200 tweets. For Joe Biden his tweets date back to almost a year ago, while Trump’s tweets only go back to July. This is reasonable because Trump is more active on Twitter.  The raw data source is provided in the CSV files that are part of our github repository (2950ProjectGitHubRepository)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* Bruin: Eric Bruin: https://www.kaggle.com/erikbruin/text-mining-the-clinton-and-trump-election-tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the data description section, the biggest limitation we faced was having a limit of ~3200 tweets from a Twitter account. This is a rule of the Twitter API, which can be bypassed by paying for premium APIs. That was the main limitation we faced, each tweet object (Tweepy Status object) fortunately contains a good amount of information that allows us to perform different and interesting analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions for Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is the plot showing logged data meaningful? How the a strong correlation in the logged plots compare in significance to data in plots that are not transformed?\n",
    "* Do you suggest any specific analysis we should perform?\n",
    "* Do you have any recommendations for us in implementing natural language processing for sentiment analysis on the Tweets we have collected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
