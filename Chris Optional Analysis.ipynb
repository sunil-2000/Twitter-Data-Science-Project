{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as axes\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden = pd.read_csv(\"biden_tweets.csv\")\n",
    "trump = pd.read_csv(\"trump_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('trump', 796), ('president', 737), ('donald', 499), ('nation', 346), ('american', 335), ('country', 306), ('people', 289), ('day', 271), ('time', 241), ('crisis', 220), ('care', 214), ('health', 212), ('help', 211), ('together', 207), ('today', 206), ('america', 197), ('americans', 183), ('work', 176), ('years', 175), ('white', 170), ('back', 168), ('know', 157), ('campaign', 151), ('better', 148), ('house', 143), ('covid-19', 142), ('world', 137), ('like', 137), ('must', 134), ('never', 132)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "from string import punctuation\n",
    "  \n",
    "allbidenwords = \"\"\n",
    "\n",
    "#concatenate texts in all of biden's post into one string, making them all lowercase\n",
    "for i in range(len(biden)):\n",
    "    allbidenwords+=biden['full_text'][i].lower()\n",
    "\n",
    "#transforms the concatenated string into a list of all the words in the string\n",
    "allbidenwords2 = allbidenwords.split()\n",
    "\n",
    "#download the nltk packages – action required running this code: go to Corpora, and download \"stopwords\"\n",
    "import nltk\n",
    "w\n",
    "#nltk.download() <--unhighlight this code to do the installation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#obtain the english stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "#create and add our own list of english stopwords\n",
    "\n",
    "additionalstopwords=['',\"—\",\"-\",\"need\",\"get\",\"make\",\"every\",\"one\",\"us\",\"take\",\"it’s\",\"it.\",\"we're\",\"i'll\",\"i'm\",\"let\",\"can't\",\"going\",\"would\"]\n",
    "\n",
    "for i in range(len(additionalstopwords)):\n",
    "    stopwords.append(additionalstopwords[i])\n",
    "    \n",
    "#remove all leading and trailing punctuations of biden's words\n",
    "for i in range(len(allbidenwords2)):\n",
    "    allbidenwords2[i]=allbidenwords2[i].strip(punctuation)\n",
    "    \n",
    "#create a new counter object which we will use to count the words and their frequencies\n",
    "\n",
    "Counter = Counter(allbidenwords2) \n",
    "\n",
    "#returns the top-200 (leave a large enough margin) most freqeunt words and their count\n",
    "bidenmostfrequentwords = Counter.most_common(200)\n",
    "\n",
    "#filter out the english stopwords from the list of words\n",
    "\n",
    "bidenmostfrequentwords = [(word, count) for word, count in bidenmostfrequentwords if word not in stopwords]\n",
    "  \n",
    "#print only the top-30 most frequent words and their count\n",
    "print(bidenmostfrequentwords[:30]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same procedure for Trump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', 200), ('amp', 194), ('biden', 131), ('people', 120), ('joe', 99), ('news', 94), ('fake', 88), ('never', 84), ('vote', 78), ('country', 74), ('big', 73), ('even', 71), ('total', 70), ('democrats', 69), ('president', 69), ('many', 65), ('left', 64), ('like', 61), ('new', 61), ('want', 60), ('back', 60), ('complete', 59), ('maga', 58), ('endorsement', 58), ('thank', 57), ('years', 56), ('state', 56), ('election', 56), ('win', 53), ('radical', 53)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "from string import punctuation\n",
    "\n",
    "alltrumpwords = \"\"\n",
    "\n",
    "#concatenate texts in all of trump's post into one string, making them all lowercase\n",
    "for i in range(len(trump)):\n",
    "    alltrumpwords+=trump['full_text'][i].lower()\n",
    "\n",
    "#transforms the concatenated string into a list of all the words in the string\n",
    "alltrumpwords2 = alltrumpwords.split()\n",
    "\n",
    "#download the nltk packages – action required running this code: go to Corpora, and download \"stopwords\"\n",
    "import nltk\n",
    "\n",
    "#nltk.download() <--unhighlight this code to do the installation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#obtain the english stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "#create and add our own list of english stopwords\n",
    "\n",
    "additionalstopwords=['',\"—\",\"-\",\"need\",\"get\",\"make\",\"every\",\"one\",\"us\",\"take\",\"it’s\",\"it.\",\"we're\",\"i'll\",\"i'm\",\"let\",\"can't\",\"going\",\"would\"]\n",
    "\n",
    "for i in range(len(additionalstopwords)):\n",
    "    stopwords.append(additionalstopwords[i])\n",
    "    \n",
    "#remove all leading and trailing punctuations of trump's words\n",
    "for i in range(len(alltrumpwords2)):\n",
    "    alltrumpwords2[i]=alltrumpwords2[i].strip(punctuation)\n",
    "    \n",
    "#create a new counter object which we will use to count the words and their frequencies\n",
    "\n",
    "Counter = Counter(alltrumpwords2) \n",
    "\n",
    "#returns the top-200 (leave a large enough margin) most freqeunt words and their count\n",
    "trumpmostfrequentwords = Counter.most_common(200)\n",
    "\n",
    "#filter out the english stopwords from the list of words\n",
    "\n",
    "trumpmostfrequentwords = [(word, count) for word, count in trumpmostfrequentwords if word not in stopwords]\n",
    "  \n",
    "#print only the top-30 most frequent words and their count\n",
    "print(trumpmostfrequentwords[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sentiment code the top 30 words from each candidates into three categories: positive, neutral, and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden's top-30 most frequent words are:\n",
      "\n",
      "[('trump', 796), ('president', 737), ('donald', 499), ('nation', 346), ('american', 335), ('country', 306), ('people', 289), ('day', 271), ('time', 241), ('crisis', 220), ('care', 214), ('health', 212), ('help', 211), ('together', 207), ('today', 206), ('america', 197), ('americans', 183), ('work', 176), ('years', 175), ('white', 170), ('back', 168), ('know', 157), ('campaign', 151), ('better', 148), ('house', 143), ('covid-19', 142), ('world', 137), ('like', 137), ('must', 134), ('never', 132)]\n",
      "\n",
      "Trump's top-30 most frequent words are:\n",
      "\n",
      "[('great', 200), ('amp', 194), ('biden', 131), ('people', 120), ('joe', 99), ('news', 94), ('fake', 88), ('never', 84), ('vote', 78), ('country', 74), ('big', 73), ('even', 71), ('total', 70), ('democrats', 69), ('president', 69), ('many', 65), ('left', 64), ('like', 61), ('new', 61), ('want', 60), ('back', 60), ('complete', 59), ('maga', 58), ('endorsement', 58), ('thank', 57), ('years', 56), ('state', 56), ('election', 56), ('win', 53), ('radical', 53)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Biden's top-30 most frequent words are:\\n\")\n",
    "print(bidenmostfrequentwords[:30])\n",
    "print(\"\\nTrump's top-30 most frequent words are:\\n\")\n",
    "print(trumpmostfrequentwords[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "16\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "bidenpositive=[\"nation\",\"american\",\"country\",\"people\",\"care\",\"health\",\"help\",\"together\",\"america\",\"americans\",\"better\",\"like\"]\n",
    "bidenneutral=[\"trump\",\"president\",\"donald\",\"day\",\"time\",\"today\",\"work\",\"years\",\"white\",\"back\",\"know\",\"campaign\",\"house\",\"covid-19\",\"world\",\"must\"]\n",
    "bidennegative=[\"crisis\",\"never\"]\n",
    "\n",
    "print(len(bidenpositive))\n",
    "print(len(bidenneutral))\n",
    "print(len(bidennegative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "13\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "trumppositive=[\"great\",\"people\",\"vote\",\"country\",\"big\",\"many\",\"like\",\"new\",\"want\",\"complete\",\"maga\",\"endorsement\",\"thank\",\"win\"]\n",
    "trumpneutral=[\"amp\",\"biden\",\"joe\",\"news\",\"even\",\"total\",\"democrats\",\"president\",\"left\",\"back\",\"years\",\"state\",\"election\"]\n",
    "trumpnegative=[\"fake\",\"never\",\"radical\"]\n",
    "\n",
    "print(len(trumppositive))\n",
    "print(len(trumpneutral))\n",
    "print(len(trumpnegative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, 12 out of the top-30 most frequently used words by Biden are positive, 16 are neutral, and 2 are negative. On the other hand, 14 out of the top-30 most frequently used words by trump are positive, 13 are neutral, and 3 are negative. This mirrors our results of sentiment analysis, which found that Trump had a lot more positive tweets than Biden (as seen through 14 > 12) as well as more negative tweets than Biden (as seen through 3 > 2). This adds further credibility to our findings that Trump's results seem more polarized and subjective than Biden's, as demonstrated in the polarity vs subjectivity plots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
